{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been a month since I had last updated this blog. During this time, I am taking a Data Science course from Udemy: Machine Learning A-Z Hands On Python and R in Data Science. [https://www.udemy.com/share/101Wci/] This course is by Kirill Eremenko and Hadelin de Ponteves. To be more precise, I am re-taking the course. I had bought the course in late 2019 and started a bit on regression and classification. Due to work committments, I stopped for a while before resuming now.\n",
    "\n",
    "I am glad I had restarted the learning. In tandem with the fast paced and continuous development in the world of Data Science, the course had been updated also. The main difference was the practical lessons. When I first started, programming lessons was conducted using Spyder. Since Jun 2020, the practical lessons had been updated to use Goodle Colaboratory in the cloud. The lessons content were also enhanced based on the questions and feedback.\n",
    "\n",
    "Another reason I liked the course is the simplicity of the explanation behind every algorithm. The instructors refrained from going into mathematics and tried using examples to simplify the intuition behind the algorithm. For my purpose of learning to be a data analyst, it helps tremendously.\n",
    "\n",
    "Enough of the batter about this course. Let's get into today's sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learning Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is simply to fit a model to predict a numerical outcome, e.g. sales volume, temperature, or price. Since the model is an approximation to the actual data, there will be deviations between the actual data and model prediction. Every model will seek to minimise this deviation in different ways. In this course, the following regression models are covered.\n",
    "\n",
    "1. Multiple Linear Regression\n",
    "2. Polynomial Linear Regression\n",
    "3. Support Vector Regression\n",
    "4. Decision Tree Regression\n",
    "5. Random Forest Regression\n",
    "\n",
    "Different models will come up with different prediction results. The classical way is to run the data through all the models with their baseline parameters and determine which model comes up with the best predictions. In this case, the R-Squared value for each model is used as the comparison metric. After the model with the highest R-Sqaured value is selected, fine tuning of the parameters can be done to improve the results.\n",
    "\n",
    "The code to be shared would run through all the method on the data and provide the R-Squared scores. Parameter tuning is not covered yet as that is the last part of the Udemy course which I had not covered. So i will update parameter tuning at a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same data preprocessing template as before. So no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for the entire code\n",
    "import numpy as np  # library for numerical manimuplation\n",
    "import matplotlib.pyplot as plt  # library for plotting\n",
    "import pandas as pd  # library for importing datasets\n",
    "\n",
    "\n",
    "def data_preprocessing(input_file_name=None,\n",
    "                       imputer_input={\"selected_col\": None,\n",
    "                                      \"strategy\": \"mean\"},\n",
    "                       x_encoder_input={\"cols\": None, \"AvoidDummy\": False},\n",
    "                       y_encoder_input=False):\n",
    "    \"\"\"\n",
    "    This is to import the dataset from the given csv file and process it for\n",
    "    subsequent analysis. The data file must be located in the same folder as\n",
    "    this python script. The processing includes 1) Data import, 2) Fill in\n",
    "    missing values, and 3) Encode categorical data.\n",
    "\n",
    "    When encoding categorical data, the new variables created based on the\n",
    "    categorical values are called dummy variables. It is important to avoid\n",
    "    using all the dummy variables (dummy variable trap) as the model\n",
    "    would be unable to differentiate the relationship. Hence, always use 1 less\n",
    "    dummy variable in the model for each set of categorical variables.\n",
    "\n",
    "    It is important to arrange the dataset columns in the following sequence\n",
    "    from left to right.\n",
    "    1. Numrical data (for filling in of missing values)\n",
    "    2. Categorical data (after encoding, it would be the first few columns in\n",
    "                          the dataset. Remove the first column to avoid dummy\n",
    "                          variable trap before encoding the next categorical\n",
    "                          variable.)\n",
    "    3. Dependent variable\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    input_file_name : This is the name of the file to be imported for analysis.\n",
    "                        The file must be located in the same folder as this\n",
    "                        python script.\n",
    "    imputer_input : A dict containing a list of columns to fill in the missing\n",
    "                    values and the strategy for the missing values.\n",
    "                    Can be \"mean\", \"median\", \"most_frequent\". If the list\n",
    "                    of columns is None, it means no missing values.\n",
    "    x_encoder_input : A dict containing a list of columns index in x to encode\n",
    "                        the categorical data and a boolen value to indicate if\n",
    "                        there is a need to avoid the dummy variable trap. An\n",
    "                        empty list indicate no need to perform encoding.\n",
    "                        Default is {cols: [], AvoidDummy=False}\n",
    "    y_encoder_input : A boolen value indicating if the dependent variable y is\n",
    "                    categorical data and need to be encoded. Default is False.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Variable containing the full data after importing from csv file.\n",
    "    imputer : SimpleImputer object created to fill in missing values\n",
    "    ct : ColumnTransformer object created to encode the categorical data in x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : Variable containing all independent features. Numpy array type\n",
    "    y : Variable containing all dependent value. Numpy array type\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Import data\n",
    "        Import from csv file\n",
    "        Extract to x and y and return them\n",
    "    Take care of missing data\n",
    "        If missing data is less than 1% of the total data, then can just\n",
    "        delete them away.\n",
    "        If there are columns with missing data\n",
    "            Replace with mean/median/most_frequent of existing data.\n",
    "    Encode categorical data\n",
    "        If there are columns in x with categorical data:\n",
    "            Use OneHotEncoder to encode each respective columns\n",
    "            OneHotEncoder will encode and transfer the dummy variables to the\n",
    "            first column of the dataset.\n",
    "        If the dependent variable y is categorical:\n",
    "            Use LabelEncoder to encode\n",
    "    Return x, y\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" Import data \"\"\"\n",
    "\n",
    "    # Read data from data file\n",
    "    dataset = pd.read_csv(input_file_name)\n",
    "    print(\"Data imported successfully.\\n\")\n",
    "\n",
    "    print(\"dataset Info\")\n",
    "    print(dataset.info(), \"\\n\")\n",
    "    print(\"dataset (First 5 rows)\")\n",
    "    print(dataset.head(), \"\\n\")\n",
    "\n",
    "    # Extract the dependent variable (y) and independent features (x).\n",
    "    # It is assumed the dependent variable column is the last one.\n",
    "    # All rows are selected. All columns except last one is selected.\n",
    "    x = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    # Reshape y into a single column\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    \"\"\" Take care of missing data \"\"\"\n",
    "    # Check if there is missing data to be filled in\n",
    "    if imputer_input[\"selected_col\"] is not None:\n",
    "        # User has provided columns index which has missing data\n",
    "        from sklearn.impute import SimpleImputer\n",
    "\n",
    "        # Create the imputer object to be used on x, y\n",
    "        imputer = SimpleImputer(missing_values=np.nan,\n",
    "                                strategy=imputer_input[\"strategy\"])\n",
    "\n",
    "        # Cycle through every column index provided by user\n",
    "        for col_index in imputer_input[\"selected_col\"]:\n",
    "            # Link the imputer object to the numerical columns in x with the\n",
    "            # correct column index\n",
    "            # Need to reshape (-1, 1) each column for the imputer object to\n",
    "            # work and reshape it back (1, -1) to put back into x.\n",
    "            imputer.fit(x[:, col_index].reshape(-1, 1))\n",
    "            # Perform the actual replacement on x\n",
    "            x[:, col_index] = \\\n",
    "                imputer.transform(x[:, col_index]\n",
    "                                  .reshape(-1, 1)).reshape(1, -1)\n",
    "\n",
    "        print(\"Missing data processed.\\n\")\n",
    "    else:\n",
    "        # The selected_col list is None. Hence no missing data.\n",
    "        print(\"Process missing data skipped. \\n\")\n",
    "\n",
    "    \"\"\" Encode categorical data \"\"\"\n",
    "    # Check if there is categorical data in x to encode\n",
    "    if x_encoder_input[\"cols\"] is not None:\n",
    "        # Column index had been provided by user to encode\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        # Cycle through each selected column index for encoding\n",
    "        for selected_col in x_encoder_input[\"cols\"]:\n",
    "            # Create ColumnTransformer object\n",
    "            ct = ColumnTransformer(transformers=[(\"encoder\",\n",
    "                                                  OneHotEncoder(),\n",
    "                                                  [selected_col])],\n",
    "                                   remainder=\"passthrough\")\n",
    "            # Fit and transform the ct object to x in one step\n",
    "            # Need to use np.array to force fit the result from fit_transform\n",
    "            # into a numpy array for subsequent analysis\n",
    "            x = np.array(ct.fit_transform(x))\n",
    "\n",
    "            # Remove first column of dummy variable to avoid dummy variable\n",
    "            # trap. The dummy variables are arranged in alphabetical order.\n",
    "            if x_encoder_input[\"AvoidDummy\"]:\n",
    "                # Need to avoid dummy variable trap because the model is unable\n",
    "                # to take care of the dummy variable\n",
    "                x = x[:, 1:]\n",
    "            else:\n",
    "                # Model is able to avoid dummy variable trap. Hence no need to\n",
    "                # remove any dummy variable.\n",
    "                pass\n",
    "\n",
    "        print(\"Categorical data encoded.\\n\")\n",
    "    else:\n",
    "        # User did not provide any columns for encoding. So do nothing.\n",
    "        print(\"Categorical data processing skipped.\")\n",
    "\n",
    "    # Check if need to encode dependent variable y\n",
    "    if y_encoder_input:\n",
    "        # Dependent variable y is categorical and need to be encoded\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "        # Create LabelEncoder object\n",
    "        le = LabelEncoder()\n",
    "        # Fit_transfer LabelEncoder onto dependent variable y\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        print(\"Categorical data in y is encoded.\\n\")\n",
    "    else:\n",
    "        # Dependent variable y is not categorical.\n",
    "        pass\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def split_dataset(x, y=None, split_data_input=[0.2, None]):\n",
    "    \"\"\"\n",
    "    This is to split the dataset into the training set and test set.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x : numpy array of independent features\n",
    "    y : a list of dependent variable. Default is None\n",
    "    split_data_input : A list containing 2 inputs required to split the\n",
    "                        dataset. [test_size=0.2, random_state=None]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train, y_train : Training set for x and y\n",
    "    x_test, y_test : Test set for x and y\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Use train_test_split from sklearn.model_selection\n",
    "    Return x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train, x_test = \\\n",
    "        train_test_split(x, test_size=split_data_input[0],\n",
    "                         random_state=split_data_input[1])\n",
    "\n",
    "    if y is not None:\n",
    "        # y is given and need to be split\n",
    "        y_train, y_test = \\\n",
    "            train_test_split(y, test_size=split_data_input[0],\n",
    "                             random_state=split_data_input[1])\n",
    "    else:\n",
    "        # y is not given\n",
    "        y_train, y_test = None, None\n",
    "\n",
    "    # Check if x_train and x_test are 2 dimensional array. Reshape if necessary\n",
    "    if len(x_train.shape) == 1:\n",
    "        # 1 dimensional list. Reshape\n",
    "        x_train = x_train.reshape(len(x_train), 1)\n",
    "        x_test = x_test.reshape(len(x_test), 1)\n",
    "    else:\n",
    "        # x_train is 2 dimensional array. Do nothing\n",
    "        pass\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def Features_Scaling(x_train, y_train=None, scaler_input=None,\n",
    "                     x_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    This function is to scale the features if required on the independent\n",
    "    variables, x. This is highly dependent on the selected model. Will be\n",
    "    using the standardisation method which is applicable to all data instead\n",
    "    of the normalisation method which can only be used on data with normal\n",
    "    distribution.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_train, y_train : Training set for x.\n",
    "    y_train : Training set for y. Assumed to be a 1 col array. Default is None.\n",
    "    scaler_input : A list containing the columns to be scaled. Default is None,\n",
    "                    which means scale all features. Only applicable for x_train\n",
    "    x_test, y_test : Test set for x and y which required to be scaled. Default\n",
    "                        is None. y_test is assumed to be a 1 column array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sc_x_train, sc_y_train : StandardScaler object created to transform the\n",
    "                                training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train_sc, y_train_sc, sc_x_train, sc_y_train, x_test_sc, y_test_sc\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Create StandardScaler object to fit and transform x_train into x_train_sc\n",
    "    If scaler_input is provided, replace columns in x_train_sc by corresponding\n",
    "    columns in x_train if the columns index is not specified in scaler_input\n",
    "    else do nothing since the entire x_train had already been transformed.\n",
    "\n",
    "    If y_train is provided:\n",
    "        If y_train is not 1D array, create StandardScaler object to fit and\n",
    "        transform y_train into y_train_sc\n",
    "    else set y_train_sc and sc_y_train to None\n",
    "\n",
    "    If x_test is provided, use sc_x_train to scale x_test into x_test_sc.\n",
    "        If scaler_input is provided, replace columns in x_test_sc by\n",
    "            corresponding columns in x_test if the columns index is not\n",
    "            specified in scaler_input.\n",
    "    else set x_test_sc to None\n",
    "\n",
    "    If y_test is provided, use sc_y_train to scale y_test into y_test_sc.\n",
    "    else set _test_sc to None\n",
    "\n",
    "    return x_train_sc, y_train_sc, sc_x_train, sc_y_train, x_test_sc, y_test_sc\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Create standard scaler object to be used on x_train\n",
    "    sc_x_train = StandardScaler()\n",
    "    x_train_sc = sc_x_train.fit_transform(x_train)\n",
    "\n",
    "    # if scaler_input is provided, only specific columns to be scaled.\n",
    "    # Replace columns in x_train_sc with x_train which need not be scaled\n",
    "    if scaler_input is not None:\n",
    "        for i in range(x_train.shape[1]):\n",
    "            # Cycle through every column in x_train\n",
    "            if i not in scaler_input:\n",
    "                x_train_sc[:, i] = x_train[:, i]\n",
    "            else:\n",
    "                # ith column needs to be scaled, so do nothing\n",
    "                pass\n",
    "    else:\n",
    "        # scaler_input not provided. So scale the entire x_train. Do nothing.\n",
    "        pass\n",
    "\n",
    "    \"\"\" Scale y_train if provided \"\"\"\n",
    "    if y_train is not None:\n",
    "        # Create StandardScaler object and fit onto y_train\n",
    "        if len(y_train.shape) == 1:\n",
    "            # y_train is 1D array. Reshape into 2D array.\n",
    "            y_train = y_train.reshape(len(y_train), 1)\n",
    "        else:\n",
    "            # y_train is not 1D array. Do nothing\n",
    "            pass\n",
    "\n",
    "        sc_y_train = StandardScaler()\n",
    "        y_train_sc = sc_y_train.fit_transform(y_train)\n",
    "    else:\n",
    "        # y_train not provided\n",
    "        y_train_sc, sc_y_train = None, None\n",
    "\n",
    "    \"\"\" Scale x_test if provided \"\"\"\n",
    "    if x_test is not None:\n",
    "        # Scale using sc_x_train\n",
    "        x_test_sc = sc_x_train.transform(x_test)\n",
    "\n",
    "        # if scaler_input is provided, only specific columns to be scaled.\n",
    "        # Replace columns in x_test_sc with x_test which need not be scaled\n",
    "        if scaler_input is not None:\n",
    "            for i in range(x_test.shape[1]):\n",
    "                # Cycle through every column in x_test\n",
    "                if i not in scaler_input:\n",
    "                    x_test_sc[:, i] = x_test[:, i]\n",
    "                else:\n",
    "                    # ith column need to be scaled. Do nothing\n",
    "                    pass\n",
    "        else:\n",
    "            # scaler_input not provided. So scale the entire x_test. Do nothing\n",
    "            pass\n",
    "    else:\n",
    "        # x_test not provided\n",
    "        x_test_sc = None\n",
    "\n",
    "    \"\"\" Scale y_test if provided \"\"\"\n",
    "    if y_test is not None:\n",
    "        # y_test is provided\n",
    "        if len(y_test.shape) == 1:\n",
    "            # y_test is 1D array. Reshape to 2D array\n",
    "            y_test = y_test.reshape(len(y_test), 1)\n",
    "        else:\n",
    "            # y_test is not 1D array. Do nothing\n",
    "            pass\n",
    "\n",
    "        # Use sc_y_train to scale y_test\n",
    "        y_test_sc = sc_y_train.transform(y_test)\n",
    "    else:\n",
    "        # y_test is not provided.\n",
    "        y_test_sc = None\n",
    "\n",
    "    return x_train_sc, y_train_sc, sc_x_train, sc_y_train,\\\n",
    "        x_test_sc, y_test_sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Regression Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main regression module which contains the code to perform Multiple Linear Regression, Decision Tree Regression, and Random Forest Regression. It uses the Scikit-Learn module as the basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression(x_train, y_train, resolution=None, reg_model=\"linear\",\n",
    "               x_test=None, y_test=None, x_pred=None,\n",
    "               x_train_plot_poly=None, ntree=0):\n",
    "    \"\"\"\n",
    "    This function uses multiple linear regression, Decision Tree Regression to\n",
    "    fit the data. It can be used for simple linear regression too.\n",
    "\n",
    "    In skelearn, the LinearRegression library would account for dummy variable\n",
    "    trap and select the variables with the highest p-value. Hence there is no\n",
    "    need to select variables to build the model.\n",
    "\n",
    "    Regression does not require feature scaling because the constant for each\n",
    "    variable would take care of the scaling.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_train, y_train : Training set for x and y.\n",
    "    resolution : No of points for a high resolution plot of the regression\n",
    "                    line. Default is 100. Applicable only to one dimensional x.\n",
    "    reg_model : Indicate the regression model to be used which includes linear,\n",
    "                decisiontree, randomforest. Default is linear.\n",
    "    x_test, y_test : Test set for x and y. Default is None if no test set is\n",
    "                        available\n",
    "    x_pred : A list containing a single sample of x for prediction\n",
    "    x_train_plot_poly : High resolution training set for x specifically for\n",
    "                        polynomial model\n",
    "    ntree : Number of trees to be used for Random Forest Regression model.\n",
    "            Default is 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : model object to fit x_train and y_train\n",
    "    x_train_plot : a high resolution list of x_train for plotting regression\n",
    "                    line.\n",
    "    y_train_reg : predicted results for dependent variable y based on x_train\n",
    "    y_test_pred : Predicted results for test data using trained model on\n",
    "                    x_test\n",
    "    y_pred : A single value prediction using x_pred\n",
    "    r2 : R2 score using y_test and y_test_pred. Used to assess if model is\n",
    "            suitable for the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train_plot, y_train_reg, y_test_pred, y_pred, r2\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Create the model object and fit to x_train, y_train\n",
    "    Print intercept and coefficients\n",
    "    If x is 1 dimensional, define high resolution x_train_plot and get high\n",
    "        resolution regression line\n",
    "    Else get regression line using x_train or x_train_plot_poly (for polynomial\n",
    "                                                                 model)\n",
    "    if x_test is provided, predict y_test_pred to compare with y_test\n",
    "    Predict for a single value x_pred if it is given\n",
    "    Return x_train_plot, y_train_reg, y_test_pred, y_pred\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    # Create model obejct based on required model\n",
    "    if reg_model == \"linear\":\n",
    "        # Linear regression\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "    elif reg_model == \"decisiontree\":\n",
    "        # Decision Tree Regression\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        model = DecisionTreeRegressor()\n",
    "    elif reg_model == \"randomforest\":\n",
    "        # Random Forest Regression\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        model = RandomForestRegressor(n_estimators=ntree)\n",
    "    else:\n",
    "        # Invalid input\n",
    "        raise ValueError(\"The selected regression model is invalid.\")\n",
    "\n",
    "    # Fit the data to the model object (training the model)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # To get the coeeficients for the simple linear regression line\n",
    "    # if reg_model == \"linear\":\n",
    "    #     print(f\"Intercept b0 = {float(model.intercept_):0.1f}\")\n",
    "    #     print(f\"Coefficient = {model.coef_}\")\n",
    "    # else:\n",
    "    #     # Not linear regression. Do nothing\n",
    "    #     pass\n",
    "\n",
    "    # if x_train is one column and resolution for curve is provided, can do\n",
    "    # high resolution regression line for y_train_pred\n",
    "    if x_train.shape[1] == 1 and resolution is not None:\n",
    "        # Only 1 dimension for x_train. Hence can do high resolution plot\n",
    "        x_train_plot = np.arange(min(x_train), max(x_train),\n",
    "                                 step=(max(x_train)-min(x_train))/resolution)\n",
    "        x_train_plot = x_train_plot.reshape(len(x_train_plot), 1)\n",
    "        y_train_reg = model.predict(x_train_plot)\n",
    "    else:\n",
    "        # x_train has multiple features. Plot regression line using original\n",
    "        x_train_plot = x_train[:]\n",
    "        if x_train_plot_poly is None:\n",
    "            # Model is not polynomial. Use original x_train for regression line\n",
    "            y_train_reg = model.predict(x_train)\n",
    "        else:\n",
    "            # Model is polynomial. Use x_train_plot_poly for regression line\n",
    "            y_train_reg = model.predict(x_train_plot_poly)\n",
    "\n",
    "    # Predict the test set results if test data is available\n",
    "    if x_test is not None:\n",
    "        # Test data available. Predict y_test_pred using x_test. This can be\n",
    "        # compared with y_test\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "    else:\n",
    "        # Test data not available.\n",
    "        y_test_pred, r2 = None, None\n",
    "\n",
    "    # To predict a value: model.predict([[value]])\n",
    "    # Take note must always put in [[]] because it expects a 2D array\n",
    "    # For Multiple Linear Regression, the inputs will be put as a list inside\n",
    "    # [[b0, b1, b2, b3...bn]]\n",
    "    if x_pred is not None:\n",
    "        # If x_pred is given, compute y_pred\n",
    "        y_pred = model.predict([x_pred])\n",
    "    else:\n",
    "        # No x_pred given, so set y_pred to None\n",
    "        y_pred = None\n",
    "\n",
    "    return x_train_plot, y_train_reg, y_test_pred, y_pred, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is used to transform the data into a polynomial format before passing to the main regression module using the multiple linear regression method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(x_train, resolution=100,\n",
    "                          x_test=None, x_pred=None, n=2):\n",
    "    \"\"\"\n",
    "    This function prepares all the independent vairables x matrixes into the\n",
    "    polynomial form for fitting the Linear Regression model.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_train : Training set for x\n",
    "    resolution : Number of points to plot for a high resolution regression line\n",
    "    x_test : Test set for x\n",
    "    x_pred : A list containing the dependent values for prediction\n",
    "    n : required degree of power for the polynomial regression model. Default\n",
    "        value is 2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    poly_reg : Polynomial model of degree n\n",
    "    x_train_plot : A numpy array holding high resolution points for x_train.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train_poly, x_test_poly, x_pred_poly : Training and test set of dependent\n",
    "                                                variable x after being\n",
    "                                                transformed into polynomial\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Create polynomial model\n",
    "    Use polynomial model on x_train, x_test and x_pred\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    # Create the polynomial model to transform x_train, x_test\n",
    "    poly_reg = PolynomialFeatures(degree=n)\n",
    "    x_train_poly = poly_reg.fit_transform(x_train)\n",
    "\n",
    "    # Define high resolution plot for x_train\n",
    "    if resolution is not None:\n",
    "        # Resolution is given. Hence can plot.\n",
    "        x_train_plot = np.arange(min(x_train), max(x_train),\n",
    "                                 step=(max(x_train)-min(x_train))/resolution)\n",
    "        x_train_plot = x_train_plot.reshape(len(x_train_plot), 1)\n",
    "        x_train_plot_poly = poly_reg.fit_transform(x_train_plot)\n",
    "    else:\n",
    "        # No need to plot\n",
    "        x_train_plot, x_train_plot_poly = None, None\n",
    "\n",
    "    if x_test is not None:\n",
    "        # x_test is provided. Transform x_test\n",
    "        x_test_poly = poly_reg.fit_transform(x_test)\n",
    "    else:\n",
    "        # x_test is not provided\n",
    "        x_test_poly = None\n",
    "\n",
    "    if x_pred is not None:\n",
    "        # x_pred is provided. Transform x_pred\n",
    "        [x_pred_poly] = poly_reg.fit_transform([x_pred])\n",
    "    else:\n",
    "        # x_pred not provided\n",
    "        x_pred_poly = None\n",
    "\n",
    "    return x_train_poly, x_train_plot, x_train_plot_poly, \\\n",
    "        x_test_poly, x_pred_poly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is used primarily for support vector regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_regression(x_train, y_train, sc_x_train, sc_y_train,\n",
    "                              resolution=None,\n",
    "                              x_test=None, y_test=None, x_pred=None):\n",
    "    \"\"\"\n",
    "    This function is to perform Support Vector Regression on the dataset. Do\n",
    "    note that feature scaling is required for the dataset as the library is\n",
    "    unable to take care of it inherently.\n",
    "\n",
    "    This implementation uses the Gaussian Radial Basis Function Kernal.\n",
    "    Refer to link in line 6 for to read up on other kernels which can be used.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_train, y_train : Training set for x and y. It must be scaled.\n",
    "    sc_x_train, sc_y_train : Standard Scaler object used to scale x_train and\n",
    "                                y_train.\n",
    "    resolution : The number of points to be used for high resolution line.\n",
    "                    Default is 100 points.\n",
    "    x_test, y_test : Test set for x and y. It must be scaled.\n",
    "                        Default is None if no test set is available\n",
    "    x_pred : A list containing the independent values for prediction. Default\n",
    "                is None.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : SVR regression object\n",
    "    x_train_plot : A list of x points for plotting the regression line.\n",
    "    y_train_reg : Predicted values using x_train.\n",
    "    y_test_pred : Predicted values using x_test. It must be scaled.\n",
    "                    Default is None if no test set is available\n",
    "    y_pred : A single prediction based on x_test\n",
    "    r2 : R2 score for the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train_plot, y_train_reg, y_test_pred, y_pred, r2\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Create SVR model and fit training data to it\n",
    "    Create high resolution plot if there is only 1 feature\n",
    "    Calculate regression values for y_train_reg using x_train_plot\n",
    "    If x_test is provided, calculate prediction for y_test_pred\n",
    "    If x_pred is provided, calculate prediction for y_pred\n",
    "    return x_train_plot, y_train_reg, y_test_pred, y_pred\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    # Create the model for training set and fit it\n",
    "    model = SVR(kernel=\"rbf\")\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Create high resolution plot if x_train has only 1 feature and resolution\n",
    "    # for the curve is provided\n",
    "    if x_train.shape[1] == 1 and resolution is not None:\n",
    "        # x_train only has one feature. Can create high resolution plot\n",
    "        x_train_plot = np.arange(min(x_train), max(x_train),\n",
    "                                 step=(max(x_train)-min(x_train))/resolution)\n",
    "        x_train_plot = x_train_plot.reshape(len(x_train_plot), 1)\n",
    "    else:\n",
    "        # x_train has more than one feature. Set x_train_plot to x_train\n",
    "        x_train_plot = x_train[:]\n",
    "\n",
    "    # Predict the regression values and reverse the scaling on y_train_reg\n",
    "    y_train_reg = sc_y_train.inverse_transform(model.predict(x_train_plot))\n",
    "    # Reverse the scaling on x_train_plot\n",
    "    x_train_plot = sc_x_train.inverse_transform(x_train_plot)\n",
    "\n",
    "    # Create model for test set and fit it, if available\n",
    "    if x_test is not None and y_test is not None:\n",
    "        # x_test and y_test are provided. Predict base on x_test.\n",
    "        y_test_pred = sc_y_train.inverse_transform(model.predict(x_test))\n",
    "        r2 = r2_score(sc_y_train.inverse_transform(y_test), y_test_pred)\n",
    "    else:\n",
    "        # x_test and y_test not provided.\n",
    "        y_test_pred, r2 = None, None\n",
    "\n",
    "    if x_pred is not None:\n",
    "        # Perform prediction. Remember to scale x_pred before the prediction\n",
    "        # and reverse the scaling for y_pred.\n",
    "        [y_pred] = sc_y_train.inverse_transform(model.predict\n",
    "                                                (sc_x_train.transform\n",
    "                                                 ([x_pred])))\n",
    "    else:\n",
    "        # x_pred not given. Return None\n",
    "        y_pred = None\n",
    "\n",
    "    return x_train_plot, y_train_reg, y_test_pred, y_pred, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module can be used for plotting high resolution regression lines if applicable. It is not activated in the main template for regression model selection. But the code to activate is embedded in the main regression template as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(x_train, y_train, x_train_plot, y_train_reg,\n",
    "                    plot_title, plot_xlabel, plot_ylabel,\n",
    "                    x_test=None, y_test=None, y_test_pred=None):\n",
    "    \"\"\"\n",
    "    This function plots the modeling results in 2 dimensional plot for\n",
    "    regression models. All the inputs must be of the original form without any\n",
    "    feature scaling.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_train, y_train : Training set for x and y\n",
    "    x_train_plot, y_train_reg : High resolution regression line\n",
    "    plot_title : title for the plot\n",
    "    plot_xlabel : x-axis label for the plot\n",
    "    plot_ylabel : y-axis label for the plot\n",
    "    x_test, y_test : Test set for x and y if available. Default is None.\n",
    "    y_test_pred : Predicted values of x_test using the model model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    if x_train has more than one column:\n",
    "        setup x_train as the case ID in x_train starting from 0\n",
    "        setup x_train_reg as the case ID in x_train_reg starting from 0 but\n",
    "            confined within the max of x_train.\n",
    "    plot x_train vs y_train in red markers\n",
    "    plot x_train_reg vs regression line (y_train_reg) in red line (high res)\n",
    "\n",
    "    if x_test and y_test are provided:\n",
    "        if x_test has more than one column:\n",
    "            setup x_test as the case ID in x_test starting from 0\n",
    "        plot x_test vs y_test in green markers\n",
    "        if y_test_reg is provided:\n",
    "            plot x_test vs y_test_reg in blue markers\n",
    "\n",
    "    Set up plot title, axis labels and legends\n",
    "    Show plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Visualise the training set results in a 2D plot\n",
    "    if x_train.shape[1] > 1:\n",
    "        # x_train has more than 1 feature. Set up case ID\n",
    "        x_train = list(i for i in range(len(x_train)))\n",
    "        x_train_plot = list(i/(len(x_train_plot)-1)*(len(x_train)-1)\n",
    "                            for i in range(len(x_train_plot)))\n",
    "    else:\n",
    "        # x_train only has 1 column. No change required\n",
    "        pass\n",
    "\n",
    "    # Plot training data as red color scatter plots\n",
    "    plt.scatter(x_train, y_train, label=\"Training Set Actual\", color=\"red\")\n",
    "\n",
    "    # Plot high res regression line in red based on training data\n",
    "    plt.plot(x_train_plot, y_train_reg, label=\"Training Set Regression\",\n",
    "             color=\"red\", linestyle=\"-\", marker=\"\")\n",
    "\n",
    "    # Visualise test set data in 2D plot if provided.\n",
    "    if x_test is not None and y_test is not None:\n",
    "        # Test data provided. Check number of features in x_test\n",
    "        if x_test.shape[1] > 1:\n",
    "            # x_test has more than 1 feature. Set up case ID\n",
    "            x_test = list(i/(len(x_test)-1)*(len(x_train)-1)\n",
    "                          for i in range(len(x_test)))\n",
    "        else:\n",
    "            # x_test has only 1 column. Do nothing\n",
    "            pass\n",
    "\n",
    "        # Plot x_test vs y_test in green markers\n",
    "        plt.scatter(x_test, y_test, label=\"Test Set Actual\", color=\"green\")\n",
    "\n",
    "        # Plot x_test vs y_test_reg in blue cross markers if available\n",
    "        if y_test_pred is not None:\n",
    "            plt.scatter(x_test, y_test_pred, label=\"Test Set Predicted\",\n",
    "                        color=\"blue\", marker=\"x\")\n",
    "        else:\n",
    "            # y_test_reg is not provided. Do nothing.\n",
    "            pass\n",
    "\n",
    "    # Set title\n",
    "    plt.title(plot_title)\n",
    "    # Set axis labels\n",
    "    plt.xlabel(plot_xlabel)\n",
    "    plt.ylabel(plot_ylabel)\n",
    "    # Display legend\n",
    "    plt.legend(loc=\"best\")\n",
    "    # Maximise the plot window\n",
    "    mng = plt.get_current_fig_manager()\n",
    "    mng.window.showMaximized()\n",
    "    # Display the graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main template to cycle through all the five regression models and find the best fitting model using the R-Squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regression_model_selection():\n",
    "    \"\"\"\n",
    "    This will be the main template to be used for regression. It will go\n",
    "    through all the regression models and compare the R2 score using all the\n",
    "    variables in the dataset. In order to select the best model, it is best to\n",
    "    go through all the models. The comparison will be using the default\n",
    "    parameters for each model. Tuning of the model is excluded from this module\n",
    "    for the time being.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R2 scores for all the models for comparison.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Data file name. File must be in same folder as script\n",
    "    dataset = \"RegressionData.csv\"\n",
    "\n",
    "    # Missing numerical data to be filled up\n",
    "    # Default values \"selected_col\": [],\"strategy\": \"mean\"\n",
    "    imputer_input = {\"selected_col\": [], \"strategy\": \"mean\"}\n",
    "\n",
    "    # Columns in x with categorical data to encode\n",
    "    # default {\"cols\": [], \"AvoidDummy\": False}\n",
    "    x_encoder_input = {\"cols\": [], \"AvoidDummy\": False}\n",
    "    # Indicate if need to encode dependent variable y, enter True or False\n",
    "    y_encoder_input = False\n",
    "\n",
    "    x, y = data_preprocessing(dataset, imputer_input, x_encoder_input,\n",
    "                              y_encoder_input)\n",
    "    print(f\"Shape of x: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "    # Inputs for splitting the dataset\n",
    "    # State the random_state as 1 for troubleshooting. Use None for random.\n",
    "    split_data_input = [0.2, None]\n",
    "\n",
    "    # Always split dataset first prior to Feature scaling because you do not\n",
    "    # want the mean value for the test set to be influenced by the training set\n",
    "    x_train, y_train, x_test, y_test = split_dataset(x, y, split_data_input)\n",
    "\n",
    "    # Set up the columns which needed to be scaled\n",
    "    # scaler_input = []\n",
    "    # Execute the scaling\n",
    "    x_train_sc, y_train_sc, sc_x_train, sc_y_train, x_test_sc, y_test_sc = \\\n",
    "        Features_Scaling(x_train, y_train, None, x_test, y_test)\n",
    "\n",
    "    # Title and labels for the plot\n",
    "    # plot_title = \"Position vs Salaries\"\n",
    "    # plot_xlabel = \"Position Level\"\n",
    "    # plot_ylabel = \"Salaries\"\n",
    "\n",
    "    # Initialise dependent value for prediction. Default is None.\n",
    "    # The scaling will be performed in SVR module.\n",
    "    # x_pred = []\n",
    "\n",
    "    # Set number of points used for high resolution plot\n",
    "    # Default is None\n",
    "    resolution = None\n",
    "\n",
    "    \"\"\"Multiple Linear Regression\"\"\"\n",
    "    reg_model = \"linear\"\n",
    "\n",
    "    # Execute module\n",
    "    x_train_plot, y_train_reg, y_test_pred, y_pred, linear_r2 = \\\n",
    "        Regression(x_train, y_train, resolution, reg_model, x_test, y_test)\n",
    "\n",
    "    # print(f\"{reg_model.title()} Regression Predicted \"\n",
    "    #       \"Value:${float(y_pred)}\")\n",
    "    print(f\"{reg_model.title()} R2: {linear_r2:0.3f}\")\n",
    "\n",
    "    \"\"\"Polynomial Linear Regression\"\"\"\n",
    "    # Initialise the expected polynomial degree\n",
    "    n = 4\n",
    "\n",
    "    # Select regression model\n",
    "    reg_model = \"linear\"\n",
    "\n",
    "    # Transform dependent variables x into polynomial format as input to Linear\n",
    "    # Regression.\n",
    "    x_train_poly, x_train_plot, x_train_plot_poly, x_test_poly, x_pred_poly = \\\n",
    "        polynomial_regression(x_train, resolution, x_test, None, n)\n",
    "\n",
    "    # Execute Linear Regression on the data\n",
    "    x_train_plot1, y_train_reg, y_test_pred, y_pred, polynomial_r2 = \\\n",
    "        Regression(x_train_poly, y_train, resolution, reg_model,\n",
    "                   x_test_poly, y_test, None, None)\n",
    "\n",
    "    # Plot results\n",
    "    # plot_regression(x, y, x_train_plot, y_train_reg,\n",
    "    #                 plot_title, plot_xlabel, plot_ylabel,\n",
    "    #                 None, None, None)\n",
    "\n",
    "    # Print y_pred if x_pred is given\n",
    "    # print(f\"For x = {x_pred}, the predicted value is ${float(y_pred):.2f}.\")\n",
    "    print(f\"Polynomial Regression R2: {polynomial_r2:0.3f}\")\n",
    "\n",
    "    \"\"\"Decision Tree Regression\"\"\"\n",
    "    reg_model = \"decisiontree\"\n",
    "\n",
    "    # Execute module\n",
    "    x_train_plot, y_train_reg, y_test_pred, y_pred, decisiontree_r2 = \\\n",
    "        Regression(x_train, y_train, resolution, reg_model, x_test, y_test)\n",
    "\n",
    "    # print(f\"{reg_model.title()} Regression Predicted \"\n",
    "    #       \"Value:${float(y_pred)}\")\n",
    "    print(f\"{reg_model.title()} R2: {decisiontree_r2:0.3f}\")\n",
    "\n",
    "    \"\"\"Random Forest Regression\"\"\"\n",
    "    reg_model = \"randomforest\"\n",
    "    # Set the number of trees if the regression model is Random Forest\n",
    "    ntree = 200\n",
    "\n",
    "    # Reshape y to row vector\n",
    "    y_train_row = y_train.ravel()\n",
    "    y_test_row = y_test.ravel()\n",
    "\n",
    "    # Execute module\n",
    "    x_train_plot, y_train_reg, y_test_pred, y_pred, randomforest_r2 = \\\n",
    "        Regression(x_train, y_train_row, resolution, reg_model,\n",
    "                   x_test, y_test_row, ntree=ntree)\n",
    "\n",
    "    # print(f\"{reg_model.title()} Regression Predicted \"\n",
    "    #       \"Value:${float(y_pred)}\")\n",
    "    print(f\"{reg_model.title()} R2: {randomforest_r2:0.3f}\")\n",
    "\n",
    "    \"\"\"Support Vector Regression\"\"\"\n",
    "    # Reshape y to row vector\n",
    "    y_train_sc = y_train_sc.ravel()\n",
    "    y_test_sc = y_test_sc.ravel()\n",
    "\n",
    "    x_train_plot, y_train_reg, y_test_pred, y_pred, svr_r2 = \\\n",
    "        support_vector_regression(x_train_sc, y_train_sc,\n",
    "                                  sc_x_train, sc_y_train,\n",
    "                                  resolution, x_test_sc, y_test_sc)\n",
    "\n",
    "    # print(f\"{reg_model.title()} Regression Predicted \"\n",
    "    #       \"Value:${float(y_pred)}\")\n",
    "    print(f\"Support Vector Regression R2: {svr_r2:0.3f}\")\n",
    "\n",
    "    # plot_regression(x, y, x_train_plot, y_train_reg,\n",
    "    #                 plot_title, plot_xlabel, plot_ylabel)\n",
    "\n",
    "    return linear_r2, polynomial_r2, decisiontree_r2, randomforest_r2, svr_r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully.\n",
      "\n",
      "dataset Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9568 entries, 0 to 9567\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      9568 non-null   float64\n",
      " 1   V       9568 non-null   float64\n",
      " 2   AP      9568 non-null   float64\n",
      " 3   RH      9568 non-null   float64\n",
      " 4   PE      9568 non-null   float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 373.9 KB\n",
      "None \n",
      "\n",
      "dataset (First 5 rows)\n",
      "      AT      V       AP     RH      PE\n",
      "0  14.96  41.76  1024.07  73.17  463.26\n",
      "1  25.18  62.96  1020.04  59.08  444.37\n",
      "2   5.11  39.40  1012.16  92.14  488.56\n",
      "3  20.86  57.32  1010.24  76.64  446.48\n",
      "4  10.82  37.50  1009.23  96.62  473.90 \n",
      "\n",
      "Missing data processed.\n",
      "\n",
      "Categorical data encoded.\n",
      "\n",
      "Shape of x: (9568, 4)\n",
      "Shape of y: (9568, 1)\n",
      "Linear R2: -0.000\n",
      "Polynomial Regression R2: -0.004\n",
      "Decisiontree R2: -1.106\n",
      "Randomforest R2: -0.124\n",
      "Support Vector Regression R2: -0.052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.00040356999342305855,\n",
       " -0.00441916122154784,\n",
       " -1.105639211544076,\n",
       " -0.12388897783167252,\n",
       " -0.052240748469885245)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model_selection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
