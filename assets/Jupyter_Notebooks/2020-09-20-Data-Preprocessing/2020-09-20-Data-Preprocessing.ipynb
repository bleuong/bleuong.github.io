{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codifying My Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of my previous posts, I had put up a data pre-processing module which I had developed as I commenced my study in Data Analytics. It was merely used as a test. Today, I shall clearly put down my rationale for the way I had written my module.\n",
    "\n",
    "First and foremost, in all the codes which I would be sharing, they are written in modular format. This means they are written as functions instead of single executable lines of code. The main reason for doing so is to allow me to reuse the code. My aim was to develop individual functions in a single file, which would allow me to utiise them in anyway in future in my data analytics work. With these modules, I can call upon any of them depending on what type of analysis I would do. That is how I am aiming to codify my knowledge and faciliate application in future work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hi(name, age):\n",
    "    \"\"\"\n",
    "    To say hi to the world.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Hello world. My name is {name} and I am {age} years old.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world. My name is John and I am 12 years old.\n"
     ]
    }
   ],
   "source": [
    "say_hi(\"John\", 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world. My name is Sally and I am 51 years old.\n"
     ]
    }
   ],
   "source": [
    "say_hi(\"Sally\", 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, I had configured my IDE (Spyder) to follow [PEP-8], which is the recommended code writing convention for Python. There are still some convention which is manually followed such as small letters separated by underscores for function names and variable names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing typically comprises of 5 main steps:\n",
    "    1. Read data into python readable format (typically is pandas)\n",
    "    2. Fill in missing data according to desired strategy (mean, median, most_frequent) (optional)\n",
    "    3. Encode categorical data in both x and y (optional)\n",
    "    4. Split data into training set and test set (optional)\n",
    "    5. Scale the data if the model is unable to handle extreme values (optional)\n",
    "\n",
    "I have bundled the first 3 steps into a single module and the remaining steps as their respective modules. The main reason is because there is a change in the data format when we start to perform Steps 4 and 5. From Steps 1 to 3, I would be working with x, y. After Step 4, I would have x_train, y_train, x_test, y_test. After Step 5, I would additionally have scaled version of the training and test set and also the scaler object to inverse the scaling. By bundling the above steps into 3 different modules, I would be able to call on any of them depending on the model to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for the entire code\n",
    "import numpy as np  # library for numerical manimuplation\n",
    "import matplotlib.pyplot as plt  # library for plotting\n",
    "import pandas as pd  # library for importing datasets\n",
    "\n",
    "def data_preprocessing(input_file_name=None,\n",
    "                       imputer_input={\"selected_col\": None,\n",
    "                                      \"strategy\": \"mean\"},\n",
    "                       x_encoder_input={\"cols\": None, \"AvoidDummy\": False},\n",
    "                       y_encoder_input=False):\n",
    "    \"\"\"\n",
    "    This is to import the dataset from the given csv file and process it for\n",
    "    subsequent analysis. The data file must be located in the same folder as\n",
    "    this python script. The processing includes 1) Data import, 2) Fill in\n",
    "    missing values, and 3) Encode categorical data.\n",
    "\n",
    "    When encoding categorical data, the new variables created based on the\n",
    "    categorical values are called dummy variables. It is important to avoid\n",
    "    using all the dummy variables (dummy variable trap) as the model\n",
    "    would be unable to differentiate the relationship. Hence, always use 1 less\n",
    "    dummy variable in the model for each set of categorical variables.\n",
    "\n",
    "    It is important to arrange the dataset columns in the following sequence\n",
    "    from left to right.\n",
    "    1. Numrical data (for filling in of missing values)\n",
    "    2. Categorical data (after encoding, it would be the first few columns in\n",
    "                          the dataset. Remove the first column to avoid dummy\n",
    "                          variable trap before encoding the next categorical\n",
    "                          variable.)\n",
    "    3. Dependent variable\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    input_file_name : This is the name of the file to be imported for analysis.\n",
    "                        The file must be located in the same folder as this\n",
    "                        python script.\n",
    "    imputer_input : A dict containing a list of columns to fill in the missing\n",
    "                    values and the strategy for the missing values.\n",
    "                    Can be \"mean\", \"median\", \"most_frequent\". If the list\n",
    "                    of columns is None, it means no missing values.\n",
    "    x_encoder_input : A dict containing a list of columns index in x to encode\n",
    "                        the categorical data and a boolen value to indicate if\n",
    "                        there is a need to avoid the dummy variable trap. An\n",
    "                        empty list indicate no need to perform encoding.\n",
    "                        Default is {cols: [], AvoidDummy=False}\n",
    "    y_encoder_input : A boolen value indicating if the dependent variable y is\n",
    "                    categorical data and need to be encoded. Default is False.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Variable containing the full data after importing from csv file.\n",
    "    imputer : SimpleImputer object created to fill in missing values\n",
    "    ct : ColumnTransformer object created to encode the categorical data in x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : Variable containing all independent features. Numpy array type\n",
    "    y : Variable containing all dependent value. Numpy array type\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Import data\n",
    "        Import from csv file\n",
    "        Extract to x and y and return them\n",
    "    Take care of missing data\n",
    "        If missing data is less than 1% of the total data, then can just\n",
    "        delete them away.\n",
    "        If there are columns with missing data\n",
    "            Replace with mean/median/most_frequent of existing data.\n",
    "    Encode categorical data\n",
    "        If there are columns in x with categorical data:\n",
    "            Use OneHotEncoder to encode each respective columns\n",
    "            OneHotEncoder will encode and transfer the dummy variables to the\n",
    "            first column of the dataset.\n",
    "        If the dependent variable y is categorical:\n",
    "            Use LabelEncoder to encode\n",
    "    Return x, y\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" Import data \"\"\"\n",
    "\n",
    "    # Read data from data file\n",
    "    dataset = pd.read_csv(input_file_name)\n",
    "    print(\"Data imported successfully.\\n\")\n",
    "\n",
    "    print(\"dataset Info\")\n",
    "    print(dataset.info(), \"\\n\")\n",
    "    print(\"dataset (First 5 rows)\")\n",
    "    print(dataset.head(), \"\\n\")\n",
    "\n",
    "    # Extract the dependent variable (y) and independent features (x).\n",
    "    # It is assumed the dependent variable column is the last one.\n",
    "    # All rows are selected. All columns except last one is selected.\n",
    "    x = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "\n",
    "    \"\"\" Take care of missing data \"\"\"\n",
    "    # Check if there is missing data to be filled in\n",
    "    if imputer_input[\"selected_col\"] is not None:\n",
    "        # User has provided columns index which has missing data\n",
    "        from sklearn.impute import SimpleImputer\n",
    "\n",
    "        # Create the imputer object to be used on x, y\n",
    "        imputer = SimpleImputer(missing_values=np.nan,\n",
    "                                strategy=imputer_input[\"strategy\"])\n",
    "\n",
    "        # Cycle through every column index provided by user\n",
    "        for col_index in imputer_input[\"selected_col\"]:\n",
    "            # Link the imputer object to the numerical columns in x with the\n",
    "            # correct column index\n",
    "            # Need to reshape (-1, 1) each column for the imputer object to\n",
    "            # work and reshape it back (1, -1) to put back into x.\n",
    "            imputer.fit(x[:, col_index].reshape(-1, 1))\n",
    "            # Perform the actual replacement on x\n",
    "            x[:, col_index] = \\\n",
    "                imputer.transform(x[:, col_index]\n",
    "                                  .reshape(-1, 1)).reshape(1, -1)\n",
    "\n",
    "        print(\"Missing data processed.\\n\")\n",
    "    else:\n",
    "        # The selected_col list is None. Hence no missing data.\n",
    "        print(\"Process missing data skipped. \\n\")\n",
    "\n",
    "    \"\"\" Encode categorical data \"\"\"\n",
    "    # Check if there is categorical data in x to encode\n",
    "    if x_encoder_input[\"cols\"] is not None:\n",
    "        # Column index had been provided by user to encode\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        # Cycle through each selected column index for encoding\n",
    "        for selected_col in x_encoder_input[\"cols\"]:\n",
    "            # Create ColumnTransformer object\n",
    "            ct = ColumnTransformer(transformers=[(\"encoder\",\n",
    "                                                  OneHotEncoder(),\n",
    "                                                  [selected_col])],\n",
    "                                   remainder=\"passthrough\")\n",
    "            # Fit and transform the ct object to x in one step\n",
    "            # Need to use np.array to force fit the result from fit_transform\n",
    "            # into a numpy array for subsequent analysis\n",
    "            x = np.array(ct.fit_transform(x))\n",
    "\n",
    "            # Remove first column of dummy variable to avoid dummy variable\n",
    "            # trap. The dummy variables are arranged in alphabetical order.\n",
    "            if x_encoder_input[\"AvoidDummy\"]:\n",
    "                # Need to avoid dummy variable trap because the model is unable\n",
    "                # to take care of the dummy variable\n",
    "                x = x[:, 1:]\n",
    "            else:\n",
    "                # Model is able to avoid dummy variable trap. Hence no need to\n",
    "                # remove any dummy variable.\n",
    "                pass\n",
    "\n",
    "        print(\"Categorical data encoded.\\n\")\n",
    "    else:\n",
    "        # User did not provide any columns for encoding. So do nothing.\n",
    "        print(\"Categorical data processing skipped.\")\n",
    "\n",
    "    # Check if need to encode dependent variable y\n",
    "    if y_encoder_input:\n",
    "        # Dependent variable y is categorical and need to be encoded\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "        # Create LabelEncoder object\n",
    "        le = LabelEncoder()\n",
    "        # Fit_transfer LabelEncoder onto dependent variable y\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        print(\"Categorical data in y is encoded.\\n\")\n",
    "    else:\n",
    "        # Dependent variable y is not categorical.\n",
    "        pass\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, split_data_input=[0.2, None]):\n",
    "    \"\"\"\n",
    "    This is to split the dataset into the training set and test set.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x : numpy array of independent features\n",
    "    y : a list of dependent variable\n",
    "    split_data_input : A list containing 2 inputs required to split the\n",
    "                        dataset. [test_size=0.2, random_state=None]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train, y_train : Training set for x and y\n",
    "    x_test, y_test : Test set for x and y\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Use train_test_split from sklearn.model_selection\n",
    "    Return x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=split_data_input[0],\n",
    "                         random_state=split_data_input[1])\n",
    "\n",
    "    # Check if x_train and x_test are 2 dimensional array. Reshape if necessary\n",
    "    if len(x_train.shape) == 1:\n",
    "        # 1 dimensional list. Reshape\n",
    "        x_train = x_train.reshape(len(x_train), 1)\n",
    "    else:\n",
    "        # x_train is 2 dimensional array. Do nothing\n",
    "        pass\n",
    "\n",
    "    if len(x_test.shape) == 1:\n",
    "        # 1 dimensional list. Reshape\n",
    "        x_test = x_test.reshape(len(x_test), 1)\n",
    "    else:\n",
    "        # x_test is 2 dimensional array. Do nothing\n",
    "        pass\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Features_Scaling(x_train, y_train=None, scaler_input=None,\n",
    "                     x_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    This function is to scale the features if required on the independent\n",
    "    variables, x. This is highly dependent on the selected model. Will be\n",
    "    using the standardisation method which is applicable to all data instead\n",
    "    of the normalisation method which can only be used on data with normal\n",
    "    distribution.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_train, y_train : Training set for x.\n",
    "    y_train : Training set for y. Assumed to be a 1 col array. Default is None.\n",
    "    scaler_input : A list containing the columns to be scaled. Default is None,\n",
    "                    which means scale all features. Only applicable for x_train\n",
    "    x_test, y_test : Test set for x and y which required to be scaled. Default\n",
    "                        is None. y_test is assumed to be a 1 column array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sc_x_train, sc_y_train : StandardScaler object created to transform the\n",
    "                                training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train_sc, y_train_sc, sc_x_train, sc_y_train, x_test_sc, y_test_sc\n",
    "\n",
    "    Pseudo Code\n",
    "    -----------\n",
    "    Create StandardScaler object to fit and transform x_train into x_train_sc\n",
    "    If scaler_input is provided, replace columns in x_train_sc by corresponding\n",
    "    columns in x_train if the columns index is not specified in scaler_input\n",
    "    else do nothing since the entire x_train had already been transformed.\n",
    "\n",
    "    If y_train is provided:\n",
    "        If y_train is not 1D array, create StandardScaler object to fit and\n",
    "        transform y_train into y_train_sc\n",
    "    else set y_train_sc and sc_y_train to None\n",
    "\n",
    "    If x_test is provided, use sc_x_train to scale x_test into x_test_sc.\n",
    "        If scaler_input is provided, replace columns in x_test_sc by\n",
    "            corresponding columns in x_test if the columns index is not\n",
    "            specified in scaler_input.\n",
    "    else set x_test_sc to None\n",
    "\n",
    "    If y_test is provided, use sc_y_train to scale y_test into y_test_sc.\n",
    "    else set _test_sc to None\n",
    "\n",
    "    return x_train_sc, y_train_sc, sc_x_train, sc_y_train, x_test_sc, y_test_sc\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Create standard scaler object to be used on x_train\n",
    "    sc_x_train = StandardScaler()\n",
    "    x_train_sc = sc_x_train.fit_transform(x_train)\n",
    "\n",
    "    # if scaler_input is provided, only specific columns to be scaled.\n",
    "    # Replace columns in x_train_sc with x_train which need not be scaled\n",
    "    if scaler_input is not None:\n",
    "        for i in range(x_train.shape[1]):\n",
    "            # Cycle through every column in x_train\n",
    "            if i not in scaler_input:\n",
    "                x_train_sc[:, i] = x_train[:, i]\n",
    "            else:\n",
    "                # ith column needs to be scaled, so do nothing\n",
    "                pass\n",
    "    else:\n",
    "        # scaler_input not provided. So scale the entire x_train. Do nothing.\n",
    "        pass\n",
    "\n",
    "    \"\"\" Scale y_train if provided \"\"\"\n",
    "    if y_train is not None:\n",
    "        # Create StandardScaler object and fit onto y_train\n",
    "        if len(y_train.shape) == 1:\n",
    "            # y_train is 1D array. Reshape into 2D array.\n",
    "            y_train = y_train.reshape(len(y_train), 1)\n",
    "        else:\n",
    "            # y_train is not 1D array. Do nothing\n",
    "            pass\n",
    "\n",
    "        sc_y_train = StandardScaler()\n",
    "        y_train_sc = sc_y_train.fit_transform(y_train)\n",
    "    else:\n",
    "        # y_train not provided\n",
    "        y_train_sc, sc_y_train = None, None\n",
    "\n",
    "    \"\"\" Scale x_test if provided \"\"\"\n",
    "    if x_test is not None:\n",
    "        # Scale using sc_x_train\n",
    "        x_test_sc = sc_x_train.transform(x_test)\n",
    "\n",
    "        # if scaler_input is provided, only specific columns to be scaled.\n",
    "        # Replace columns in x_test_sc with x_test which need not be scaled\n",
    "        if scaler_input is not None:\n",
    "            for i in range(x_test.shape[1]):\n",
    "                # Cycle through every column in x_test\n",
    "                if i not in scaler_input:\n",
    "                    x_test_sc[:, i] = x_test[:, i]\n",
    "                else:\n",
    "                    # ith column need to be scaled. Do nothing\n",
    "                    pass\n",
    "        else:\n",
    "            # scaler_input not provided. So scale the entire x_test. Do nothing\n",
    "            pass\n",
    "    else:\n",
    "        # x_test not provided\n",
    "        x_test_sc = None\n",
    "\n",
    "    \"\"\" Scale y_test if provided \"\"\"\n",
    "    if y_test is not None:\n",
    "        # y_test is provided\n",
    "        if len(y_test.shape) == 1:\n",
    "            # y_test is 1D array. Reshape to 2D array\n",
    "            y_test = y_test.reshape(len(y_test), 1)\n",
    "        else:\n",
    "            # y_test is not 1D array. Do nothing\n",
    "            pass\n",
    "\n",
    "        # Use sc_y_train to scale y_test\n",
    "        y_test_sc = sc_y_train.transform(y_test)\n",
    "    else:\n",
    "        # y_test is not provided.\n",
    "        y_test_sc = None\n",
    "\n",
    "    return x_train_sc, y_train_sc, sc_x_train, sc_y_train,\\\n",
    "        x_test_sc, y_test_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Control Module to Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Data file name. File must be in same folder as script\n",
    "    dataset = \"sample.csv\"\n",
    "    # Missing numerical data to be filled up\n",
    "    # Default values \"selected_col\": [],\"strategy\": \"mean\"\n",
    "    imputer_input = {\"selected_col\": [], \"strategy\": \"mean\"}\n",
    "    # Columns in x with categorical data to encode, default {\"cols\": [],\n",
    "    #                                                   \"AvoidDummy\": False}\n",
    "    x_encoder_input = {\"cols\": [], \"AvoidDummy\": False}\n",
    "    # Indicate if need to encode dependent variable y, enter True or False\n",
    "    y_encoder_input = False\n",
    "\n",
    "    x, y = data_preprocessing(dataset, imputer_input, x_encoder_input,\n",
    "                              y_encoder_input)\n",
    "\n",
    "    # Inputs for splitting the dataset\n",
    "    # State the random_state as 1 for troubleshooting. Use None for random.\n",
    "    split_data_input = [0.2, 0]\n",
    "\n",
    "    # Always split dataset first prior to Feature scaling because you do not\n",
    "    # want the mean value for the test set to be influenced by the training set\n",
    "    x_train, y_train, x_test, y_test = split_dataset(x, y, split_data_input)\n",
    "\n",
    "    # Set up the columns which needed to be scaled\n",
    "    # scaler_input = []\n",
    "    # # Execute the scaling\n",
    "    # x_train_sc, y_train_sc, sc_x_train, sc_y_train, x_test_sc, y_test_sc = \\\n",
    "    #     Features_Scaling(x_train, None, scaler_input, x_test, None)\n",
    "\n",
    "    print(\"x_train\\n\", x_train)\n",
    "    print(\"y_train\\n\", y_train)\n",
    "    print(\"x_test\\n\", x_test)\n",
    "    print(\"y_test\\n\", y_test)\n",
    "    \n",
    "    # To continue with model building (to be covered in subsequent posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully.\n",
      "\n",
      "dataset Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   YearsExperience  30 non-null     float64\n",
      " 1   Salary           30 non-null     float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 608.0 bytes\n",
      "None \n",
      "\n",
      "dataset (First 5 rows)\n",
      "   YearsExperience   Salary\n",
      "0              1.1  39343.0\n",
      "1              1.3  46205.0\n",
      "2              1.5  37731.0\n",
      "3              2.0  43525.0\n",
      "4              2.2  39891.0 \n",
      "\n",
      "Missing data processed.\n",
      "\n",
      "Categorical data encoded.\n",
      "\n",
      "x_train\n",
      " [[ 9.6]\n",
      " [ 4. ]\n",
      " [ 5.3]\n",
      " [ 7.9]\n",
      " [ 2.9]\n",
      " [ 5.1]\n",
      " [ 3.2]\n",
      " [ 4.5]\n",
      " [ 8.2]\n",
      " [ 6.8]\n",
      " [ 1.3]\n",
      " [10.5]\n",
      " [ 3. ]\n",
      " [ 2.2]\n",
      " [ 5.9]\n",
      " [ 6. ]\n",
      " [ 3.7]\n",
      " [ 3.2]\n",
      " [ 9. ]\n",
      " [ 2. ]\n",
      " [ 1.1]\n",
      " [ 7.1]\n",
      " [ 4.9]\n",
      " [ 4. ]]\n",
      "y_train\n",
      " [112635.  55794.  83088. 101302.  56642.  66029.  64445.  61111. 113812.\n",
      "  91738.  46205. 121872.  60150.  39891.  81363.  93940.  57189.  54445.\n",
      " 105582.  43525.  39343.  98273.  67938.  56957.]\n",
      "x_test\n",
      " [[ 1.5]\n",
      " [10.3]\n",
      " [ 4.1]\n",
      " [ 3.9]\n",
      " [ 9.5]\n",
      " [ 8.7]]\n",
      "y_test\n",
      " [ 37731. 122391.  57081.  63218. 116969. 109431.]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing these modules, the arguements to be passed between the modules must be scrutinised. Other than data type, the other concern is the shape of the pandas array used to store the data. Different models may have different requirements for the shape, whether it is a one column or one row array. Sometimes, the models can accommodate to it and would only give a warning. Otherwise, it may cause the program to malfunction. So, do consider to include the shape of the array as part of the docstring to remind oneself to take note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PEP-8]: https://www.python.org/dev/peps/pep-0008/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
